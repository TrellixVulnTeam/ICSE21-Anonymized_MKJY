---
title: "style-analyzer: reproduction artifact for ICSE 2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This document guides through the reproduction of the paper:

  Vadim Markovtsev, Hugo Mougard, Waren Long, Egor Bulychev, Konstantin Slavnov.  
  *Style-analyzer: fixing code style inconsistencies with interpretable unsupervised algorithms.*  
  MSR 2019. DOI: 10.1109/MSR.2019.00073  
  https://arxiv.org/pdf/1904.00935.pdf  
  
The artifact can be used to reproduce the entire experiment from scratch or just 
to reproduce graphs and numbers in the paper from pre-generated data.

The artifact contains the following systems:

  - `QUERY_ROOT` contains two CODEDJ queries: to select projects for the 
    experiment and to generate a list of projects in a format usable by 
    STYLE ANALYZER 
  - `EXPERIMENT_ROOT` contains the STYLE ANALYZER reproduction package and 
    scripts for post-processing the data
  - this document co-ordinates execution and provides 
  
We do not provide a copy of CODEDJ. We assume it is installed and set up so that 
PARASITE can be executed via a commandline command `parasite`. CODEDJ is 
available from `https://codedj-prg.github.io`.  
  
The artifact contains the following data:  

  - `DJANCO_DIR` contains the output returned by CODEDJ: 
      - `selections`: URLs of selected projects
      - `specs`: URLs of selected projects + head and base commit hashes for 
         each
      - `info`: contains basic information about selected projects, most 
         importantly: locs
              
  - `REPRO_DIR` contains the output of style analyzer; each subdirectory is a 
    separate project sample, each containing the following:
      - an MD report for each succesfully examined project
      - a couple of summary MD reports for the selection as a group
      - a `logs.txt` file generated while running the experiment
    
    In addition the directory also contains pre-generated results of 
    post-processing: csv files extracting information from MD files and logs
    and turning them into a form that is easy to digest by this document.

  - `FIG_DIR` contains the figures generated by this document.
  
This artifact does not include the initial CODEDJ dataset needed to make project 
selections, due to size constraints. The dataset can be obtained from <TODO>
  
Table of contents:

  1. **Paths** - set up paths to various elements of the artifact
  2. **Datasets** - run CODEDJ queries and generate project selections 
     for the experiment
  3. **Experiment** - run the STYLE ANALYZER benchmark on various project 
     selections
  4. **Post-processing and loading** - ingest the results of the experiments 
     into R
  5. **Visualization** - visualize the results, generate figures and export
     numbers to a LaTeX file
     
To re-generate the figures run **1 -> 4 -> 5**. This only takes a few minutes.

To re-run the experiment from existing project selections run 
**1 -> 3 -> 4 -> 5**. This can take upwards of a day.

To run the entire pipeline, run all sections in order. This can take upwards of
a week.
  
# Paths

A list of important directories with descriptions of each.

```{r}
# This is the root of the arifact. All paths are relative to this path. 
PROJECT_ROOT=getwd()

# The path where this document is.
NOTEBOOK_ROOT=paste0(PROJECT_ROOT, "/reproductions/style_analyzer/")

# The path where the CODEDJ query is. This is the root of a runnable Rust repo.
QUERY_ROOT=paste0(NOTEBOOK_ROOT, "/style-analyzer-query/")

# The path of the experiment. This is the root directory from which STYLE 
# ANALYZER scripts are executed.
EXPERIMENT_ROOT=paste0(NOTEBOOK_ROOT, "/style-analyzer-repro/")

# This is where te experiment outputs logs and reports. This is used by this 
# document as inputs.
REPRO_DIR=paste0(NOTEBOOK_ROOT, "/reproductions/")

# This is where CODEDJ outputs csv files. This docuemnt uses the contents as 
# inputs.
DJANCO_DIR=paste0(QUERY_ROOT, "/output/")

# This is where this document outputs figures it generates.
FIG_DIR=paste0(PROJECT_ROOT, "/figs/style-analyzer/")

# WARNING! ALL DIRS MUST HAVE '/' AT THE END!
```

Initialize a log that will collect numbers generated by the document and make 
a file that can be included in LaTeX.

```{r}
setwd(PROJECT_ROOT)
source("artifact/latex-log.R")
initialize_log(name = "style_analyzer")
```

# Datasets

```r
setwd(QUERY_ROOT)
```

Use a CODEDJ dataset of JavaScript projects to generate a selection. We use 
PARASITE v3.0. The dataset contains all projects in subset GENERIC. The dataset 
should be installed at path: `/mnt/data/codedj-icse/javascript/`.

```bash
cargo run --bin select --release -- \
  --output-path output \
  --dataset-path /mnt/data/codedj-icse/javascript/ \
  --cache-path /mnt/data/codedj-icse/javascript-cache/
```

We create another dataset with just the projects we selected. The user needs to 
generate a GITHUB personal token and put it in a CSV file `ghtokens.csv` 
(important: must have a column header). For more info see 
`https://docs.github.com/en/github/authenticating-to-github/creating-a-personal-access-token`
and `https://codedj-prg.github.io/`.

```bash
# Create a list of all unique projects in the selections.
cat output/selections/*.csv | sort | uniq | head -n -1 | tail -n +2 \
  > output/selections/all.csv

# Prepare for running parasite
mkdir -p /mnt/data/codedj-icse/js-selection/
mkdir -p /mnt/data/codedj-icse/js-selection/repo_clones

# Add projects to datastore
parasite --datastore /mnt/data/codedj-icse/js-selection/ \
  add output/selections/all.csv

# Run parasite (updateall)
parasite --datastore /mnt/data/codedj-icse/js-selection/ \
  -ght ghtokens.csv -n 8 --interactive

# Merge the dataset
parasite --datastore /mnt/data/codedj-icse/js-selection-merged/ \
  --merge-all /mnt/data/codedj-icse/js-selection Generic
```

Generate head and base commits for each project.

```bash
cargo run --bin generate_experiment_inputs --release -- \
  --output-path output \
  --dataset-path /mnt/data/codedj-icse/js-selection-merged/ \
  --cache-path /mnt/data/codedj-icse/js-selection-merged-cache/
```

```r
setwd(PROJECT_ROOT)
```

# Experiment

```r
setwd(EXPERIMENT_ROOT)
```

Use the STYLE ANALYZER experiment framework (converted from original paper)
to run the tool on each project selection. 

## Prerequisites

```bash
# I don't know if we actually need these, probably not.
sudo apt install python3 python3-pip
pip3 install pip install git+git://github.com/snowballstem/pystemmer
pip3 install bblfsh

# Docker
sudo curl -sSL https://get.docker.com/ | sh
sudo chmod 666 /var/run/docker.sock

# Rust and djanco
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
cargo install --git https://github.com/PRL-PRG/cargo-djanco
```

## Experiment

Start the BBLFSH parser docker container. The experiment uses this as a service 
to parse JS files.

```bash
make bblfsh-start
```

Run experiment. The make file will start a docker container which 

```bash
make run-experiment
```

Run each selection separately:

```bash
# Project selection from original paper, HEAD and BASE commits from original 
# paper.
make run-experiment selections="original_reproduction"

# Project selection from original paper, HEAD is newest default branch commit,
# BASE is commit at 10% offset from HEAD.
make run-experiment selections="original"

# Project selections from CODEDJ query, HEAD is newest default branch commit,
# BASE is commit at 10% offset from HEAD.
make run-experiment selections="quality_projects_0"
make run-experiment selections="quality_projects_1"
make run-experiment selections="quality_projects_2"
make run-experiment selections="quality_projects_3"
make run-experiment selections="quality_projects_4"
make run-experiment selections="quality_projects_5"
make run-experiment selections="quality_projects_6"
make run-experiment selections="quality_projects_7"
make run-experiment selections="quality_projects_8"
make run-experiment selections="quality_projects_9"
```

STYLE ANALYZER can be temperamental, and we found that re-running a selection 
was enough to get it to finish, or produce a missing summary report. 
Nevertheless, STYLE ANALYZER can have a big error rate, so if a repository
is giving you trouble, especially hanging up the pipeline, remove it from the 
selection -- there are extras since we expect failures. Sorry, we're dealing 
with somebody else's research-grade software, so it likes to go boing and 
sometimes we don't understand why.

```r
setwd(PROJECT_ROOT)
```

# Post-processing and loading

This post-processes the report as given by the STYLE ANALYZER benchmark and
loads them into R for use with this document. It depends on either the existence 
of the pre-generated documents, or on the execution of the previous sections of 
the document.

```{R}
library(readr)
library(dplyr)
library(stringr)
library(kableExtra)
library(ggplot2)
library(scales)
library(cowplot)
library(gridExtra)
library(ggridges)
library(ggpubr)
library(tidyr)
library(digest)
```

Setting up paths. The experiments to post-process are hard-coded here. 
If one or more selections didn't finish processing, they can be removed from 
further processing here.

```{r}
# This is where figures are generated into
dir.create(FIG_DIR, recursive=TRUE)

# A list of all reproductions, these are subdirectories of REPRO_DIR. These 
# should not have spaces in them. They are used as names, they should not 
# have / in them.
quality_projects <- paste0("quality_projects_", c(0:9))
EXPERIMENTS <- c("original_reproduction", "original", quality_projects)
PAPER_EXPERIMENTS <- c("original", quality_projects)

# Convert to bash variables
Sys.setenv(REPRO_DIRS = paste(paste0(REPRO_DIR, "/", EXPERIMENTS), collapse = " "), PROJECT_ROOT = PROJECT_ROOT, EXPERIMENT_ROOT = EXPERIMENT_ROOT)
```

## Extract information from reproductions

Extract data form logs and reports into easily digestible CSV files. This calls 
bash and AWK scripts. The reports and logs from the previous step must be 
present.

```{bash}
cd "$EXPERIMENT_ROOT"

# We execute this once for every experiment.
for REPRO_DIR in $REPRO_DIRS 
do
  echo "Analyzing $REPRO_DIR"

  # Calculate prediction rates by analyzing the log.
  scripts/extract_prediction_rate.awk \
      <"${REPRO_DIR}/logs.txt" \
      >"${REPRO_DIR}/predictions.csv" 
  
  # Convert summary test report into CSV.
  scripts/extract_summary.awk \
      <"${REPRO_DIR}/summary-test_report.md" \
      >"${REPRO_DIR}/summary.csv" \
  || echo "Failed to generate summary.csv for ${REPRO_DIR}: this means the selection didn't finish processing." \
          "It only matters if you want to generate the big table."
  
  # Extract unique label counts from individual reports.
  scripts/extract_number_of_labels.sh \
       "${REPRO_DIR}/" \
      >"${REPRO_DIR}/labels.csv" 
      
  # Extract support and full support unmbers for training data.
  scripts/extract_training_samples.sh \
       "${REPRO_DIR}/" \
      >"${REPRO_DIR}/training_samples.csv" 
      
  # Extract support and full support unmbers for training data.
  scripts/extract_test_precision.sh \
       "${REPRO_DIR}/" \
      >"${REPRO_DIR}/precision.csv" 
done

cd "$PROJECT_ROOT"
```


## Collate the data

Ingest the post-processed data into R.

```{r}
# Some helper functions
url_to_repo <- function(url) str_replace_all(url, c("(^.*/)|([.]git$)"), "")

# Common: all projects and their locs
all_projects_path <- paste0(DJANCO_DIR, "info/javascript_projects.csv")
all_projects_extended_path <- paste0(DJANCO_DIR, "info/javascript_projects_extended.csv")
project_locs_path <- paste0(DJANCO_DIR, "info/project_locs.csv")
 
# Fix: these are now part of all projects 
# Common: from small datatset containing only the projects from the paper
#project_locs_path_repro <- paste0(DJANCO_DIR, "info/repro_project_locs.csv")
#all_projects_path_repro <- paste0(DJANCO_DIR, "info/repro_javascript_projects.csv")

# Load common data: this is only data for the 1000-or-so projects we use for analysis
all_projects <- read_csv(all_projects_path) %>% mutate(repo = url_to_repo(url)) %>% mutate(url = str_replace(url, ".git$", ""))
all_projects_extended <- read_csv(all_projects_extended_path) %>% mutate(repo = url_to_repo(url)) %>% mutate(url = str_replace(url, ".git$", ""))
project_locs <- read_csv(project_locs_path) %>% mutate(repo = url_to_repo(url)) %>% mutate(url = str_replace(url, ".git$", ""))

# Fix: these are now part of all projects 
# Repro extras: attach the data from small dataset to the big dataset
# all_projects_repro <- read_csv(all_projects_path_repro) %>% mutate(repo = url_to_repo(url))
# project_locs_repro <- read_csv(project_locs_path_repro) %>% mutate(repo = url_to_repo(url))
# all_projects <- bind_rows(all_projects, all_projects_repro) %>% distinct
# project_locs <- bind_rows(project_locs, project_locs_repro) %>% distinct

# Construct a table with the results of one experiment
prepare_data <- function(experiment, size=19) {
  cat(paste0("Preparing results for ", experiment))
  
  # Paths
  predictions_path <- paste0(REPRO_DIR, experiment, "/predictions.csv")
  summary_path <- paste0(REPRO_DIR, experiment, "/summary.csv")
  labels_path <- paste0(REPRO_DIR, experiment, "/labels.csv")
  samples_path <- paste0(REPRO_DIR, experiment, "/training_samples.csv")
  precision_path <- paste0(REPRO_DIR, experiment, "/precision.csv")

  # CSVs
  predictions <- read_csv(predictions_path) %>% mutate(url = str_replace(url, ".git$", "")) # there is one in project_locs and this one has a different format
  labels <- read_csv(labels_path) %>% mutate(repo = url_to_repo(repo))
  samples <- read_csv(samples_path) %>% mutate(repo = url_to_repo(repo)) %>% rename(report_training_samples=support, report_full_training_samples=full_support)
  precision <- read_csv(precision_path) %>% mutate(repo = url_to_repo(repo)) %>% rename(report_precision=precision)

  # Summary is optional, since we're concentrating on graphs not tables.
  # If the whole selection didn't finish processing, the summary won't be generated, 
  # but everything else will be.
  finished_processing <- file.exists(summary_path);
  summary <-
    if (finished_processing) read_csv(summary_path) %>% mutate(repo = url_to_repo(repo))
    else tibble(repo=character(0), precision=numeric(0), 
                recall=numeric(0), full_recall=numeric(0), 
                f1=numeric(0), full_f1=numeric(0), 
                ppcr=integer(0), 
                support=integer(0), full_support=integer(0), 
                `Rules Number`=integer(0), `Average Rule Len`=numeric(0))
  
  # Collate results 
  collated <- predictions %>%
    left_join(project_locs, by=c("url", "repo")) %>%
    left_join(summary, by="repo") %>%
    left_join(labels, by="repo") %>%
    left_join(samples, by="repo") %>%
    left_join(precision, by="repo")
   
  #browser()  
  # Format the output
  data <- collated %>%
    select(-recall, -f1, -support, -precision) %>%
    mutate(pred_r=predictions/samples) %>%
    rename(recall=full_recall, 
           precision=report_precision,
           f1=full_f1,
           support=report_training_samples,
           avg_rule_len=`Average Rule Len`,
           rules=`Rules Number`) %>%
    select(url, repo, precision, pred_r, 
           recall, f1, support, locs,
           labels, rules, avg_rule_len,  
           training_time) %>%
    filter(!is.na(precision)) %>%
    distinct
    
  print(paste0((data %>% count)$n, " projects in ", experiment))
  
  if ((data %>% count)$n > size) {
    seed <- digest::digest2int(experiment, seed=1111)
    set.seed(seed)
    data %>% sample_n(size)
  } else {
    data
  }
}

# Prepare data for all experiments. Put the results in a list.
experiment_data <- lapply(EXPERIMENTS, prepare_data) 
# prepare_data("top_starred_projects_5")
names(experiment_data) <- EXPERIMENTS

experiment_data[["paper"]] <- tibble(
  repo       = c("node", "webpack", "meteor", "react", "atom", "react-native", "jquery", "storybook", "freeCodeCamp", "express", "30-seconds-of-code", "evergreen", "citgm", "axios", "create-react-app", "redux", "reveal.js", "carlo", "telescope"),
  precision  = c(0.965,  0.957,     0.9,      0.943,   0.955,   0.94,          0.972,    0.94,        0.928,           0.937,    0.951,                0.894,       0.936,    0.94,   0.895,              0.937,    0.897,      0.878,   0.806),
  support    = c(374298, 358012,    337627,   304465,  265521,  264961,        197072,   161366,      114020,          78411,    67737,                38387,       21941,    21130,  16718,              14783,    9974,       5529,    731)
)

min_precision = sapply(experiment_data, function(data) min(data$precision)) %>% min
max_support = sapply(experiment_data, function(data) max(data$support)) %>% max

for (experiment in experiment_data) {
  print(experiment)
}
```

# Paper

This section creates the figures used in the paper as well as a file full of 
LaTeX commands that generate numbers that get included into the paper.

There are two figures:
  - `precision-vs-samples` Fig.8 Relationship between label groups and precision
  - `precision-and-samples-comparison` Fig.9 Difference in precision
  
## Selection renaming  
  
Before the figures are created we label the selections in a slightly cleaner,
slightly more human readable way. The labels will still look presentable even if
we remove some selections from the pipeline.

```{r}
stars_sequence <- 0
quality_sequence <- 0
next_stars <- function () {
  stars_sequence <<- stars_sequence + 1 
  stars_sequence
}
next_quality <- function () {
  quality_sequence <<- quality_sequence + 1 
  quality_sequence
}
make_selection_label <- function(experiment) {
  selection_name <- if (str_starts(experiment, "top_starred_projects")) { paste0("Top Stars ", next_stars()) } 
               else if (str_starts(experiment, "quality_projects")) { paste0("Interesting ", next_quality()) } 
               else if (str_starts(experiment, "original")) { "Original" } 
               else experiment
  selection_name
}
selection_names <- sapply(PAPER_EXPERIMENTS, make_selection_label, simplify=FALSE)
selection_name_order <- c("Original", paste0(" Top Stars ", 1:stars_sequence), paste0(" Interesting Projects ", 1:quality_sequence, " "))
strip_and_label <- function(data, label) {
  selection_name <- selection_names[[label]]
  data %>% 
    mutate(selection=label, selection_name=selection_name) %>% 
    select(selection, selection_name, repo, precision, support, 
           precision_similarity, support_similarity)
}
```

## Figure 8: Precision vs support (project size)

This plots 4 scatterplots from: one from the `original` selection and 3 from 
whichever the first three `quality_project_$i` selections are. The figure is 
shown here and simultaneously copied into a PDF file in `FIG_DIR`.

```{r fig.width=5, fig.height=5}
make_precision_support_graph <- function (experiment, uniform_scales=TRUE, labels=FALSE, color_by_selection=FALSE, log_scale=FALSE, only_top_n=NULL,  only_bottom_n=NULL, use_selection_name=FALSE, left=T, bottom=T) {
  data <- experiment_data[[experiment]]
  if (!is.null(only_top_n)) {
    data <- data %>% mutate(i=1:n()) %>% top_n(only_top_n, i)
    print(data)
  }
  if (!is.null(only_bottom_n)) {
    data <- data %>% mutate(i=1:n()) %>% top_n(-only_bottom_n, i)
    print(data)
  }
  plot <- 
  if (color_by_selection) {
    ggplot(data, aes(x=precision, y=support, alpha=0.2, color=selection)) + geom_point(size=2, shape=4)
  } else {
    ggplot(data, aes(x=precision, y=support, alpha=0.2)) + geom_point(size=2, shape=4)
  }
  if (labels) {
    plot <- plot + geom_text(aes(label=repo), position=position_nudge(y=-10000),  size=3)
  }
  
  x_range <- if(uniform_scales) c(min_precision, 1) else NULL
  y_range <- if(uniform_scales) c(0 - 10000, max_support) else NULL
  
  if (log_scale) {
    plot <- plot + scale_y_log10(labels=label_number_si(), limits = y_range)
  } else {
    plot <- plot + scale_y_continuous(limits = y_range, breaks=c(250000, 500000, 750000, 1000000), labels=c("250k", "500k", "750k", "1m"))
  }
  
  plot <- plot +
    scale_x_continuous(labels=function(x) format(x, digits=2), limits = x_range) +
    xlab("Precision") + 
    ylab("Project size") +
    #ggtitle(ifelse(use_selection_name, selection_names[[experiment]], paste0("Selection: ", experiment))) +
    theme_bw() + 
    theme(legend.position="none", panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
    theme(panel.border=element_blank(), axis.line.x = element_line(size = 0.2), axis.line.y = element_line(size = 0.2)) + 
    theme(plot.title=element_text(margin=margin(t=10,b=-20), hjust = 0.05)) +
    theme(axis.title.y = element_blank())  +
    theme(axis.title.x = element_blank()) 
  
  plot
}

no_bottom <- function(plot)  plot + theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())
no_left <- function(plot)  plot + theme(axis.text.y = element_blank(), axis.ticks.y = element_blank())

y_text <- text_grob("Project size", rot=90, size=12)
x_text <- text_grob("Precision", size=12)

grid.arrange(
  make_precision_support_graph("original", use_selection_name = T) %>% no_bottom,
  make_precision_support_graph("quality_projects_0", use_selection_name = T) %>% no_bottom %>% no_left,
  make_precision_support_graph("quality_projects_1", use_selection_name = T),
  make_precision_support_graph("quality_projects_2", use_selection_name = T) %>% no_left,
  ncol=2, nrow=2, left=y_text, bottom=x_text, heights=c(5,5.25), widths=c(5.65,5)
)

dev.copy(pdf, file=paste0(FIG_DIR, "/precision-vs-samples.pdf"), height=5, width=5)
dev.off()
```

## Figure 9: Precision distribution

This plots the precision from each selection on a box plot. In addition, this
snippet compares each distribution to the orignal using R's `wilcoxon` function 
(in practice Mann Whitley's U test) and uses that to color the bars in the 
figure. The figure is shown here and simultaneously copied into a PDF file in 
`FIG_DIR`.

```{r, fig.width=5, fig.height=3}
stars_sequence <- 0
quality_sequence <- 0
next_stars <- function () {
  stars_sequence <<- stars_sequence + 1 
  stars_sequence
}
next_quality <- function () {
  quality_sequence <<- quality_sequence + 1 
  quality_sequence
}
make_selection_label <- function(experiment) {
  selection_name <- if (str_starts(experiment, "top_starred_projects")) { paste0("Top Stars ", next_stars()) } 
               else if (str_starts(experiment, "quality_projects")) { paste0("Interesting ", next_quality()) } 
               else if (str_starts(experiment, "original")) { "Original" } 
               else experiment
  selection_name
}
selection_names <- sapply(PAPER_EXPERIMENTS, make_selection_label, simplify=FALSE)
selection_name_order <- c("Original", paste0(" Top Stars ", 1:stars_sequence), paste0(" Interesting Projects ", 1:quality_sequence, " "))
strip_and_label <- function(data, label) {
  selection_name <- selection_names[[label]]
  data %>% mutate(selection=label, selection_name=selection_name) %>% select(selection, selection_name, repo, precision, support, precision_similarity, support_similarity)
}

mann_whitney_significance <- function(experiment) {
  precision_result <- wilcox.test(experiment_data$original$precision, experiment_data[[experiment]]$precision, exact=FALSE)
  support_result <- wilcox.test(experiment_data$original$support, experiment_data[[experiment]]$support, exact=FALSE)
  experiment_data[[experiment]] %>% 
    mutate(precision_similarity = ifelse (experiment == "original", "original", ifelse(precision_result$p.value > 0.05, "equivalent to original", "different"))) %>%
    mutate(support_similarity = ifelse (experiment == "original", "original", ifelse(support_result$p.value > 0.05, "equivalent to original", "different")))
}
significance <- sapply(PAPER_EXPERIMENTS, mann_whitney_significance, simplify=FALSE)
significance <- sapply(significance, function(x) {
  x %>% 
    mutate(precision_similarity=factor(precision_similarity, levels=c("original","equivalent to original", "different"))) %>%
    mutate(support_similarity=factor(support_similarity, levels=c("original","equivalent to original", "different"))) 
}, simplify=FALSE)

stripped_and_labelled <- lapply(PAPER_EXPERIMENTS, function(experiment) significance[[experiment]] %>% strip_and_label(experiment))
comparison_data <- do.call(bind_rows, stripped_and_labelled) %>% mutate(selection_name = factor(selection_name, levels=selection_name_order))

original_precision_median <- comparison_data %>% filter(selection=="original") %>% pull(precision) %>% median

color_original = "#9A32CD" #"#D7263D" #"#DB2763" #"#0BB19F"
color_different = "#DB2763" 
color_equivalent = "#574AE2"  #"#0BB19F"

precision_comparison <- ggplot(comparison_data, aes(x=selection, y=precision, color=precision_similarity, fill=precision_similarity, alpha=precision_similarity)) +
  geom_boxplot(width = 0.65, position = position_dodge(width=0.05), outlier.shape = 4, outlier.alpha = 1) + 
  geom_vline(xintercept=1.5, color="black", linetype="dotted", size=0.35) +
  geom_hline(yintercept=original_precision_median, linetype="dotted", color = color_original, size=0.35) + # small projects median    //red
  xlab("Selection") + ylab("Precision") + 
  theme_bw() + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.border=element_blank()) +
  theme(legend.title = element_blank(), axis.ticks.x = element_blank(), axis.title.x = element_blank()) +
  theme(legend.text=element_text(size=8), legend.key.size = unit(9, "pt"),, legend.position = c(0.5, 0.2), legend.background = element_blank()) +
  scale_y_continuous(limits=c(0.6,1)) + #, ) +
  scale_alpha_manual(values=rep(0.5, 3)) + 
  scale_fill_manual(values=c(color_original, color_equivalent, color_different)) +
  scale_color_manual(values=c(color_original, color_equivalent, color_different)) +
  scale_x_discrete(labels=c("Original", "","","","","Interesting Projects", "", "", "", "", "", "")) 

precision_comparison

dev.copy(pdf, file=paste0(FIG_DIR, "/precision-and-samples-comparison.pdf"), height=3, width=5)
dev.off()
```

In addition to the figure, we count the number of significantly different 
selections and report them in the paper.

```{r}
similar_selections <- comparison_data %>% group_by(selection, precision_similarity) %>% summarize() %>% ungroup() %>% filter(selection != "original") %>% group_by(precision_similarity) %>% count()

equivalent <- similar_selections %>% filter(precision_similarity=="equivalent to original") %>% pull(n)
different <- similar_selections %>% filter(precision_similarity=="different") %>% pull(n)

LOG("SAEquivalentSelections", equivalent)
LOG("SADifferentSelections", different)
LOG("SAAllSelections", equivalent + different)
```

## Table like from the original paper:

These tables are a reproduction from the STYLE ANALYZER paper. They are not used
in our paper, but we report some of the numbers, which we LOG out.

```{r}
weighed_mean_precisions <- list()
make_paper_table <- function(experiment, latex = FALSE, digits=3) {
  
  data <- experiment_data[[experiment]] %>% mutate(`training time, min` = round(training_time/60))
  
  stringified <-
    data %>% 
    mutate(repo = repo, 
           precision = format(precision, digits=digits),
           pred_r = format(pred_r, digits=digits),
           recall = format(recall, digits=digits),
           f1 = format(f1, digits=digits),
           support = format(support, digits=digits),
           locs = format(locs, digits=digits),
           rules = format(rules, digits=digits),
           avg_rule_len = format(avg_rule_len, digits=digits),
           labels = format(labels, digits=digits),
           training_time = format(training_time, digits=digits)) %>%
    arrange(desc(support))

  average <- 
    data %>% 
    summarize(repo = "average", 
              precision = format(mean(precision), digits=digits),
              pred_r = format(mean(pred_r), digits=digits),
              recall = format(mean(recall), digits=digits),
              f1 = format(mean(f1), digits=digits),
              support = format(mean(support), digits=digits),
              locs = NA,
              rules = format(mean(rules), digits=digits),
              avg_rule_len = format(mean(avg_rule_len), digits=digits),
              labels = format(mean(labels), digits=digits),
              training_time = NA)
  
  weighted_average <- 
    data %>% 
    summarize(repo = "weighted average", 
              precision = format(weighted.mean(precision, support), digits=digits),
              pred_r = format(weighted.mean(pred_r, support),  digits=digits),
              recall = format(weighted.mean(recall, support), digits=digits),
              f1 = format(weighted.mean(f1, support), digits=digits),
              locs = NA,
              rules = NA,
              avg_rule_len = NA,
              labels = NA,
              training_time = NA,
              support = NA)
  
  weighted_mean_precision <- list(weighted.mean(data$precision, data$support))
  names(weighted_mean_precision) <- experiment
  weighed_mean_precisions <<- append(weighed_mean_precisions,
                                     weighted_mean_precision)
  
  table <- bind_rows(stringified, average, weighted_average) %>%
    rename(repository=repo,
           `PredR`=pred_r,
           `train samples` = support,
           `LoC` = locs,
           `unique labels` = labels,
           `avg. rule length` = avg_rule_len) %>%
    select(`repository`, `precision`, `PredR`, 
           `recall`, `f1`, `train samples`, `LoC`, 
           `unique labels`, `rules`, `avg. rule length`, 
           `training time, min`)
  
  k <- kable(table, booktabs = TRUE, format = ifelse(latex, "latex", "html"), caption = paste0("Selection: ", experiment)) %>%
    kable_styling(latex_options = "striped") %>%
    row_spec((table %>% count)$n - 3, hline_after = TRUE) # Add hline before average and weighted average
  
  return(k)
}

for (experiment in EXPERIMENTS) {
  print(make_paper_table(experiment))
}
```

Print out the numbers we report in the paper from the tables.

```{r}
original_weighed_mean_preicions <-
  weighed_mean_precisions[names(weighed_mean_precisions) != "original" & 
                          names(weighed_mean_precisions) != "original_reproduction"]

LOG("SAMinPrecision", format(min(unlist(original_weighed_mean_preicions)), digits=2))
LOG("SAMaxPrecision", format(max(unlist(original_weighed_mean_preicions)), digits=2))
```

## Reproduction qualiy check

This section compares the quality of the reproduction by checking how the 
precision and support attributes diverge between the `paper` and 
`original_reproduction` selections. `original_reproduction` is the exact 
selection, heads and base commits as in the paper, but we ran STYLE ANALYZER on
it ourselves with our settings. `paper` is just the numbers reported in the 
paper. This shows what the difference is from just re-running the experiment.

The plot is merely instructive.

```{r}
comparison <- inner_join(experiment_data$paper, experiment_data$original_reproduction, by="repo")
ggplot() + 
  geom_segment(comparison, mapping=aes(x=precision.x, xend=precision.y, y=support.x, yend=support.y, alpha=0.2, color=repo), arrow=arrow()) + 
  geom_text(data=comparison, aes(x=precision.x, y=support.x, label=repo)) + 
  theme_bw() +  
  scale_x_continuous(labels=function(x) format(x, digits=2)) + #, limits = c(min_precision, 1)) +
  scale_y_log10(labels=label_number_si()) + #limits =  c(0 - 10000, max_support)) +
  ggtitle("Paper vs reproduction using input from artifact") + # (same, just bigger and not scaled like everything else)") +
  xlab("Precision") +
  ylab("Samples in training set") +
  theme(legend.position="none")

```

We analyze the results and report some of the differences in the paper.

```{r}

deltas <- comparison %>% 
  mutate(precision=precision.y,
         precision_delta=abs(precision.y-precision.x), 
         precision_delta_perc=format(100*precision_delta/precision.x, digits=1),
         support=support.y,
         support_delta=abs(support.y-support.x),
         support_delta_perc=format(100*support_delta/support.x, digits=1)) %>% 
  select(repo, precision, precision_delta, precision_delta_perc, support, support_delta,  support_delta_perc)

deltas_summary <- deltas %>% summarize(
  mean_precision_delta=mean(as.numeric(precision_delta)),
  max_precision_delta=max(as.numeric(precision_delta)),
  mean_support_delta=mean(as.numeric(support_delta)),
  max_support_delta=max(as.numeric(support_delta)),
  mean_precision_delta_perc=mean(as.numeric(precision_delta_perc)),
  max_precision_delta_perc=max(as.numeric(precision_delta_perc)),
  mean_support_delta_perc=mean(as.numeric(support_delta_perc)),
  max_support_delta_perc=max(as.numeric(support_delta_perc)))

LOG("SAMeanPrecisionDeltaPaperVOriginal", paste0(format(deltas_summary$mean_precision_delta_perc, digits=2), "\\%"))
LOG("SAMeanSupportDeltaPaperVOriginal", paste0(format(deltas_summary$mean_support_delta_perc, digits=2), "\\%"))
LOG("SAMaxPrecisionDeltaPaperVOriginal", paste0(format(deltas_summary$max_precision_delta_perc, digits=2), "\\%"))
LOG("SAMaxSupportDeltaPaperVOriginal", paste0(format(deltas_summary$max_support_delta_perc, digits=2),  "\\%"))
```

## Quality of head and base commit selection

This section compares the quality of the reproduction by checking how the 
precision and support attributes diverge between the `orignal` and 
`original_reproduction` selections. `original_reproduction` is the exact 
selection, heads and base commits as in the paper. The `original` selection 
has the same projects, but different heads and base commits.  This shows what 
differences we get by swapping out the heads and bases.

The plot is merely instructive.

```{r}
comparison <- inner_join(experiment_data$original_reproduction, experiment_data$original, by="repo")
ggplot() + 
  geom_segment(comparison, mapping=aes(x=precision.x, xend=precision.y, y=support.x, yend=support.y, alpha=0.2, color=repo), arrow=arrow()) + 
  geom_text(data=comparison, aes(x=precision.x, y=support.x, label=repo)) + 
  theme_bw() +  
  scale_x_continuous(labels=function(x) format(x, digits=2)) + #, limits = c(min_precision, 1)) +
  scale_y_log10(labels=label_number_si()) + #limits =  c(0 - 10000, max_support)) +
  ggtitle("Original project set: using original head and base vs using our generated ones") +
  xlab("Precision") +
  ylab("Samples in training set") +
  theme(legend.position="none")
```  

We generate a few numbers that we use in the paper to talk about these 
differences.

```{r}
deltas <- comparison %>% 
  mutate(precision=precision.y,
         precision_delta=precision.y-precision.x, 
         precision_delta_perc=format(100*abs(precision_delta)/precision.x, digits=1),
         support=support.y,
         support_delta=support.y-support.x,
         support_delta_perc=format(100*abs(support_delta)/support.x, digits=1)) %>% 
  select(repo, precision, precision_delta, precision_delta_perc, support, support_delta,  support_delta_perc)

deltas_summary <- deltas %>% summarize(
  mean_precision_delta=mean(as.numeric(precision_delta)),
  max_precision_delta=max(as.numeric(precision_delta)),
  mean_support_delta=mean(as.numeric(support_delta)),
  max_support_delta=max(as.numeric(support_delta)),
  mean_precision_delta_perc=mean(as.numeric(precision_delta_perc)),
  max_precision_delta_perc=max(as.numeric(precision_delta_perc)),
  mean_support_delta_perc=mean(as.numeric(support_delta_perc)),
  max_support_delta_perc=max(as.numeric(support_delta_perc)))

LOG("SAMeanPrecisionDeltaOriginalVRepro", paste0(format(deltas_summary$mean_precision_delta_perc, digits=2), "\\%"))
LOG("SAMeanSupportDeltaOriginalVRepro", paste0(format(deltas_summary$mean_support_delta_perc, digits=2), "\\%"))
LOG("SAMaxPrecisionDeltaOriginalVRepro", paste0(format(deltas_summary$max_precision_delta_perc, digits=2), "\\%"))
LOG("SAMaxSupportDeltaOriginalVRepro", paste0(format(deltas_summary$max_support_delta_perc, digits=2), "\\%"))
```

## Flush log

Output all generated numbers into a LaTeX file.

```{r}
flush_log()
```
